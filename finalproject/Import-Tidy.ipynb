{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT RUN THIS SCRIPT\n",
    "============\n",
    "Takes 15-25 minutes to run if database has not been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Summary\n",
    "=====================\n",
    "\n",
    "The original Netflix Prize dataset was in the form of a small file for items (movies, tv shows, etc.) that had one line per item, and several test file for validating the model that reasearches had developed for the competition. The main training data was located a folder that contained 1 file per item in the item file. The item file had 17770 entries, and the training data folder had 17770 files containing an arbitary amount of ratings per file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datafile Formats\n",
    "=====================\n",
    "\n",
    "Item File Format\n",
    "----------------------\n",
    "Each line has the format:\n",
    "\n",
    "MovieID,YearOfRelease,Title\n",
    "\n",
    "- MovieID do not correspond to actual Netflix movie ids or IMDB movie ids.\n",
    "- YearOfRelease can range from 1890 to 2005 and may correspond to the release of\n",
    "  corresponding DVD, not necessarily its theaterical release.\n",
    "- Title is the Netflix movie title and may not correspond to \n",
    "  titles used on other sites.  Titles are in English.\n",
    "\n",
    "Rating File Format\n",
    "----------------------\n",
    "Each file has a single line to start that contains the MovieID followed by a colon. After that each line has the format:\n",
    "\n",
    "CustomerID,Rating,Date\n",
    "\n",
    "- MovieIDs range from 1 to 17770 sequentially.\n",
    "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import/Tidy\n",
    "============\n",
    "\n",
    "The general method for import was:\n",
    " 1. Open connection to sqlite3 file\n",
    " 2. Delete schemas if present\n",
    " 3. Create schemas\n",
    " 4. Parse item file\n",
    " 5. Commit to databse\n",
    " 6. For each rating file parse rating file and commit to database\n",
    "\n",
    "Tidying was an artifact of placing the data in the database, the following items were cleaned:\n",
    " - Dates were read from format and placed in ISO format, sqlite supports sorting date strings in ISO format\n",
    " - Stripping textual data of leading and trailing whitespace\n",
    " - Converting numeric data to appropriate type, int or real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pathlib as plib\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DB_DEFINITION = \"\"\"\n",
    "CREATE TABLE item\n",
    "(\n",
    "    item_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    release_year INTEGER,\n",
    "    name TEXT NOT NULL,\n",
    "    avg_rating REAL DEFAULT 0 NOT NULL,\n",
    "    rating_count INT DEFAULT 0 NOT NULL,\n",
    "    mean_day REAL DEFAULT 0 NOT NULL\n",
    ");\n",
    "CREATE TABLE rating\n",
    "(\n",
    "    rating_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    item_id INTEGER NOT NULL,\n",
    "    customer_id INTEGER NOT NULL,\n",
    "    value REAL NOT NULL,\n",
    "    baseline_value REAL DEFAULT 0.0 NOT NULL,\n",
    "    date TEXT NOT NULL,\n",
    "    day INTEGER NOT NULL,\n",
    "    model_group_id INTEGER NOT NULL\n",
    ");\n",
    "CREATE TABLE customer\n",
    "(\n",
    "    customer_id INT PRIMARY KEY NOT NULL,\n",
    "    avg_rating REAL NOT NULL,\n",
    "    rating_count INT NOT NULL,\n",
    "    mean_day REAL NOT NULL\n",
    ");\n",
    "CREATE TABLE customer_time_freq\n",
    "(\n",
    "    customer_id INT NOT NULL,\n",
    "    day INT NOT NULL,\n",
    "    freq INT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "CLEAR_DB = \"\"\"\n",
    "DROP TABLE IF EXISTS item;\n",
    "DROP TABLE IF EXISTS rating;\n",
    "DROP TABLE IF EXISTS customer;\n",
    "DROP TABLE IF EXISTS customer_time_freq;\n",
    "\"\"\"\n",
    "\n",
    "INSERT_ITEM = \"\"\"\n",
    "INSERT INTO\n",
    "    item (item_id, release_year, name)\n",
    "VALUES\n",
    "    (?, ?, ?);\n",
    "\"\"\"\n",
    "INSERT_RATING = \"\"\"\n",
    "INSERT INTO\n",
    "    rating (item_id, customer_id, value, 'date', day, model_group_id) \n",
    "VALUES\n",
    "    (?, ?, ?, ?, ?, ?);\n",
    "\"\"\"\n",
    "INSERT_CUSTOMERS = \"\"\"\n",
    "INSERT INTO\n",
    "    customer (customer_id, avg_rating, rating_count, mean_day)\n",
    "SELECT\n",
    "    customer_id, avg(value), count(value), avg(day)\n",
    "FROM rating\n",
    "GROUP BY customer_id;\n",
    "\"\"\"\n",
    "INSERT_CUSTOMER_DAY_FREQS = \"\"\"\n",
    "INSERT INTO \n",
    "    customer_time_freq (customer_id, day, freq)\n",
    "SELECT\n",
    "    customer_id, day, count(*) as freq\n",
    "FROM rating\n",
    "GROUP BY customer_id, day;\n",
    "\"\"\"\n",
    "CREATE_TEMP_ITEM_STATS = \"\"\"\n",
    "CREATE TEMP TABLE item_stats\n",
    "(\n",
    "    item_id INT PRIMARY KEY NOT NULL,\n",
    "    avg_rating REAL NOT NULL,\n",
    "    rating_count INT NOT NULL,\n",
    "    mean_day REAL NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "FILL_TEMP_ITEM_STATS = \"\"\"\n",
    "INSERT INTO\n",
    "    item_stats (item_id, avg_rating, rating_count, mean_day)\n",
    "SELECT\n",
    "    item_id, avg(value), count(value), avg(day)\n",
    "FROM rating\n",
    "GROUP BY item_id;\n",
    "\"\"\"\n",
    "CREATE_TEMP_ITEM_STATS_INDEX = \"\"\"\n",
    "CREATE INDEX item_id_temp_stats_index ON item_stats (item_id);\n",
    "\"\"\"\n",
    "INSERT_ITEM_STATS = \"\"\"\n",
    "UPDATE item\n",
    "SET\n",
    "    avg_rating = (\n",
    "        SELECT\n",
    "            item_stats.avg_rating\n",
    "        FROM item_stats\n",
    "        WHERE item_stats.item_id = item.item_id\n",
    "    ),\n",
    "    rating_count = (\n",
    "        SELECT\n",
    "            item_stats.rating_count\n",
    "        FROM item_stats\n",
    "        WHERE item_stats.item_id = item.item_id\n",
    "    ),\n",
    "    mean_day = (\n",
    "        SELECT\n",
    "            item_stats.mean_day\n",
    "        FROM item_stats\n",
    "        WHERE item_stats.item_id = item.item_id\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "SHIFT_ITEM_ID_DOWN = \"\"\"\n",
    "UPDATE item\n",
    "SET item_id = item_id - 1;\n",
    "UPDATE rating\n",
    "SET item_id = item_id - 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DATA_HOME = plib.Path('/') / 'data' / 'declanvk'\n",
    "RAW_DATA_FOLDER = PROJECT_DATA_HOME / 'netflix_prize'\n",
    "TRAINING_DATA_FOLDER = RAW_DATA_FOLDER / 'training_set'\n",
    "ITEM_FILE = RAW_DATA_FOLDER / 'movie_titles.txt'\n",
    "DB_FILE = PROJECT_DATA_HOME / 'netflix_prize.sqlite'\n",
    "\n",
    "MAX_DATE = datetime.strptime(\"2005-12-31\", \"%Y-%m-%d\")\n",
    "MIN_DATE = datetime.strptime(\"1999-11-11\", \"%Y-%m-%d\")\n",
    "DATE_RANGE = MAX_DATE - MIN_DATE\n",
    "SUB_GROUPS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main_import():\n",
    "    \"\"\"Main function\"\"\"\n",
    "\n",
    "    with sqlite3.connect(str(DB_FILE)) as conn:\n",
    "        curs = conn.cursor()\n",
    "        try:\n",
    "            curs.executescript(CLEAR_DB)\n",
    "            curs.executescript(DB_DEFINITION)\n",
    "            conn.commit()\n",
    "            print(\"Cleared database and loaded table definitions\")\n",
    "\n",
    "            import_items(curs)\n",
    "            conn.commit()\n",
    "            print(\"Imported all items\")\n",
    "\n",
    "            import_all_ratings(curs, conn)\n",
    "            conn.commit()\n",
    "            print(\"Imported all ratings\")\n",
    "\n",
    "            curs.execute(INSERT_CUSTOMERS)\n",
    "            conn.commit()\n",
    "            print(\"Collected customer data from ratings\")\n",
    "            \n",
    "            curs.execute(INSERT_CUSTOMER_DAY_FREQS)\n",
    "            conn.commit()\n",
    "            print(\"Insert customer day frequencies\")\n",
    "\n",
    "            curs.execute(CREATE_TEMP_ITEM_STATS)\n",
    "            curs.execute(FILL_TEMP_ITEM_STATS)\n",
    "            curs.execute(CREATE_TEMP_ITEM_STATS_INDEX)\n",
    "            curs.execute(INSERT_ITEM_STATS)\n",
    "            conn.commit()\n",
    "            print(\"Collected pre item statistiscs\")\n",
    "\n",
    "            curs.executescript(SHIFT_ITEM_ID_DOWN)\n",
    "            conn.commit()\n",
    "            print(\"Shifted item_id down to account for later tensor indexing use\")\n",
    "        except Exception as err:\n",
    "            conn.rollback()\n",
    "            raise err\n",
    "        finally:\n",
    "            curs.close()\n",
    "\n",
    "def import_items(cursor, item_file_path=ITEM_FILE):\n",
    "    \"\"\"Import items from dataset\"\"\"\n",
    "    with item_file_path.open('r', encoding='latin1') as item_file:\n",
    "        items = map(parse_item_row, item_file)\n",
    "        cursor.executemany(INSERT_ITEM, items)\n",
    "\n",
    "def import_all_ratings(cursor, connection, training_data_path=TRAINING_DATA_FOLDER):\n",
    "    \"\"\"Import all rating files in training_data_path and remaps customer ids\"\"\"\n",
    "    training_files = list(training_data_path.iterdir())\n",
    "    customer_id_mapping = dict()\n",
    "    current_id = 0\n",
    "    for idx, training_file in enumerate(training_files):\n",
    "        current_id = import_rating(cursor, training_file, customer_id_mapping, current_id)\n",
    "        connection.commit()\n",
    "\n",
    "def import_rating(cursor, training_file, id_mapping, current_id):\n",
    "    \"\"\"Import single rating file\"\"\"\n",
    "    ratings_df = pd.read_csv(str(training_file), skiprows=1, header=None)\n",
    "    item_id = int(training_file.name[3:10])\n",
    "    ratings_df[2] = pd.to_datetime(ratings_df[2], format=\"%Y-%m-%d\")\n",
    "    ratings_df[3] = (ratings_df[2] - MIN_DATE).apply(lambda td: td.days)\n",
    "    ratings_df[4] = np.random.randint(SUB_GROUPS, size=ratings_df.shape[0])\n",
    "\n",
    "    def create_tuple(df_row):\n",
    "        \"\"\"Process rows of rating file\"\"\"\n",
    "        nonlocal current_id\n",
    "        nonlocal item_id\n",
    "        if df_row[0] not in id_mapping:\n",
    "            id_mapping[df_row[0]] = current_id\n",
    "            current_id = current_id + 1\n",
    "\n",
    "        new_id = id_mapping[df_row[0]]\n",
    "        return item_id, new_id, float(df_row[1]),\\\n",
    "                str(df_row[2].date()), int(df_row[3]), int(df_row[4])\n",
    "\n",
    "    ratings = map(create_tuple, ratings_df.itertuples(index=False, name=None))\n",
    "\n",
    "    cursor.executemany(INSERT_RATING, ratings)\n",
    "\n",
    "    return current_id\n",
    "\n",
    "def parse_item_row(item_row_string):\n",
    "    \"\"\"Read items from file contents\"\"\"\n",
    "    item_row = item_row_string.strip().split(\",\")\n",
    "    item_id = int(item_row[0])\n",
    "    release_year = datetime.strptime(item_row[1], \"%Y\").date().year \\\n",
    "                    if item_row[1] != 'NULL' else None\n",
    "    name = \",\".join(item_row[2:])\n",
    "    return item_id, release_year, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not DB_FILE.exists():\n",
    "    main_import()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Data & Precomputing Statistics\n",
    "================================\n",
    "\n",
    "Certain commonly used pieces of data were precomputed, such as the averages for each item and customer, and the number of ratings per each item or customer. Another field that was added was a random number that will make sampling the database slightly easier. Instead of needing to perform an very expensive random selection based on RatingID, I can randomly select from the ModelGroupID field which is a randomly assigned number between 0 and 1,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Indices\n",
    "=========\n",
    "\n",
    "I specifically kept indices out of the schema while I imported the data. After the databse was completed on the server I added an index to the following fields:\n",
    " - Item\n",
    "   - ItemID aka MovieID\n",
    "   - Name\n",
    " - Rating\n",
    "   - RatingID\n",
    "   - ItemID aka MovieID\n",
    "   - CustomerID\n",
    "   - ModelGroupID\n",
    " - Customer\n",
    "   - CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS item_id_index ON item (item_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS name_item_index ON item (name);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS rating_id_index ON rating (rating_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS item_id_rating_index ON rating (item_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS custome_id_rating_index ON rating (customer_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS value_rating_index ON rating (value);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS day_customer_rating_index ON rating (customer_id, day);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS day_item_rating_index ON rating (item_id, day);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS date_rating_index ON rating (date);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS day_rating_index ON rating (day);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS model_group_id_rating_index ON rating (model_group_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS customer_id_customer_index ON customer (customer_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS customer_id_customer_time_freq_index ON customer_time_freq (customer_id);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS day_customer_time_freq_index ON customer_time_freq (day);\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"CREATE INDEX IF NOT EXISTS freq_customer_time_freq_index ON customer_time_freq (freq);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"ANALYZE;\"\n",
    "!sqlite3 /data/declanvk/netflix_prize.sqlite \"VACUUM;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Size\n",
    "==============\n",
    "\n",
    "The training data alone is roughly 2.4 gigabytes on my computer, imported into the sqlite db without indices it is roughly 2.8 gigabytes, after adding additional tables and precomputing counts and averages it's 4.1 gigabytes, and on the server with indices added the sqlite file is roughly 16 gigabytes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
